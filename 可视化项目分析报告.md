# 项目分析报告：Dataview 数据可视化平台

## 1. 项目概览

**Dataview** 是一个全栈数据可视化与分析平台，专为处理时间序列数据而设计。它提供了一个直观的 Web 界面，允许用户上传 CSV 数据集，进行交互式图表展示、数据统计分析以及手动标注功能。

该项目采用了经典的前后端分离架构，后端基于 Python Flask 提供 RESTful API 和数据处理能力，前端基于 React 构建响应式用户界面。

## 2. 技术栈

### 2.1 前端 (Frontend)

- **核心框架**: React 18
- **构建工具**: Vite
- **UI 组件库**: Ant Design (AntD) 5.x
- **样式方案**: Tailwind CSS + Less
- **图表库**: ECharts (echarts-for-react)
- **网络请求**: Axios
- **日期处理**: Day.js

### 2.2 后端 (Backend)

- **Web 框架**: Flask 3.0
- **数据库 ORM**: Flask-SQLAlchemy
- **数据处理**: Pandas (用于高效 CSV 解析与处理)
- **跨域处理**: Flask-Cors
- **数据库**: SQLite (轻量级，支持 WAL 模式)

## 3. 项目结构

```
dataview/
├── backend/                  # 后端服务代码
│   ├── app.py               # 主应用入口，包含 API 定义与模型
│   ├── dataview.db          # SQLite 数据库文件
│   ├── requirements.txt     # Python 依赖清单
│   ├── uploads/             # 上传文件存储目录
│   └── temp_chunks/         # 断点续传临时目录
├── frontend/                 # 前端应用代码
│   ├── src/
│   │   ├── components/      # React 组件 (Header, Charts, etc.)
│   │   ├── styles/          # 全局样式
│   │   ├── App.jsx          # 主应用逻辑
│   │   └── main.jsx         # 入口文件
│   ├── package.json         # 前端依赖配置
│   ├── tailwind.config.js   # Tailwind 配置
│   └── vite.config.js       # Vite 构建配置
├── PROJECT_REPORT.md         # 项目分析报告
├── README.md                 # 项目说明文档
└── start.sh                  # 启动脚本
```

## 4. 核心功能模块

### 4.1 数据集管理

- **文件上传**: 支持大文件分片上传 (Chunked Upload) 和断点续传，确保大 CSV 文件上传的稳定性。
- **后台处理**: 上传完成后，后台异步线程使用 Pandas 解析 CSV，清洗数据并存入数据库。
- **状态追踪**: 实时反馈数据集状态（待处理、处理中、就绪、失败）。

### 4.2 数据可视化

- **交互式图表**: 使用 ECharts 渲染高性能时间序列折线图。
- **动态筛选**: 支持按时间范围缩放 (DataZoom) 和指标 (Metric) 筛选。
- **降采样策略**: 后端根据数据量自动执行降采样 (Downsampling)，保证前端渲染性能。

### 4.3 数据标注系统

- **可视化标注**: 用户可以在图表上框选时间范围进行标注。
- **标注管理**: 支持创建、编辑、删除标注，包含内容、状态（信息/警告/错误）和颜色自定义。
- **持久化**: 标注数据与数据集关联存储在数据库中。

### 4.4 统计分析

- **基础统计**: 自动计算各指标的最小值、最大值、平均值和样本数。
- **数据导出**: 支持将过滤后的数据导出为 CSV 文件。

## 5. 数据库设计

项目使用 SQLite 数据库，包含以下三个核心实体：

### 5.1 Dataset (数据集)

| 字段       | 类型     | 说明                                      |
| ---------- | -------- | ----------------------------------------- |
| id         | Integer  | 主键                                      |
| filename   | String   | 原始文件名                                |
| created_at | DateTime | 创建时间                                  |
| status     | String   | 状态 (pending, processing, ready, failed) |

### 5.2 DataPoint (数据点)

| 字段       | 类型     | 说明                         |
| ---------- | -------- | ---------------------------- |
| id         | Integer  | 主键                         |
| dataset_id | Integer  | 外键 (关联 Dataset)          |
| timestamp  | DateTime | 时间戳 (索引)                |
| metric     | String   | 指标名称 (如 temp, pressure) |
| value      | Float    | 数值                         |

### 5.3 Annotation (标注)

| 字段       | 类型     | 说明                |
| ---------- | -------- | ------------------- |
| id         | Integer  | 主键                |
| dataset_id | Integer  | 外键 (关联 Dataset) |
| start_time | DateTime | 开始时间            |
| end_time   | DateTime | 结束时间            |
| content    | String   | 标注内容            |
| status     | String   | 标注类型            |
| color      | String   | 显示颜色            |

## 6. API 接口文档

### 数据集相关

- `POST /api/upload/chunk`: 上传文件分片
- `POST /api/upload/merge`: 合并分片并触发处理
- `GET /api/datasets`: 获取所有数据集列表
- `DELETE /api/datasets/<id>`: 删除数据集

### 数据查询

- `GET /api/data/<id>`: 获取时序数据 (支持 `start`, `end`, `metric` 参数)
- `GET /api/stats/<id>`: 获取数据集统计信息
- `GET /api/download/<id>`: 导出数据 CSV

### 标注相关

- `GET /api/annotations/<dataset_id>`: 获取标注列表
- `POST /api/annotations`: 创建新标注
- `PUT /api/annotations/<id>`: 更新标注
- `DELETE /api/annotations/<id>`: 删除标注

## 7. 部署与运行

### 7.1 环境要求

- Node.js >= 16
- Python >= 3.8

### 7.2 启动步骤

1. **后端启动**:

   ```bash
   cd backend
   pip install -r requirements.txt
   python app.py
   # 服务运行在 http://localhost:5001
   ```

2. **前端启动**:
   ```bash
   cd frontend
   npm install
   npm run dev
   # 页面访问 http://localhost:5173
   ```

## 8. 未来规划 (Roadmap)

### 8.1 性能与架构升级

- **数据库迁移**: 从 SQLite 迁移至专业的时间序列数据库 (如 TimescaleDB 或 InfluxDB)，以支持亿级数据点的秒级查询。
- **高性能计算**: 引入 Celery + Redis 替换当前的简单线程池，实现更健壮的异步任务队列和分布式处理。
- **存储优化**: 支持 Parquet 列式存储格式，提高大规模历史数据的读取效率和压缩率。

### 8.2 功能扩展

- **高级分析**: 集成 Scikit-learn 或 Prophet，增加趋势预测、异常检测和自动聚类功能。
- **多维分析**: 支持多 CSV 关联分析、自定义计算字段（公式编辑器）以及更多图表类型（热力图、散点矩阵）。
- **多用户协作**: 增加用户认证 (OAuth/JWT) 和基于角色的权限控制 (RBAC)，支持团队协作标注与分享。

### 8.3 工程化与运维

- **容器化部署**: 提供 Dockerfile 和 Docker Compose 配置，实现一键部署。
- **质量保障**: 完善后端 PyTest 单元测试和前端 Cypress E2E 测试，建立 CI/CD 流水线。
- **监控告警**: 集成 Prometheus 和 Grafana，监控系统性能指标（CPU、内存、API 延迟）。

### 8.4 用户体验优化

- **移动端适配**: 优化移动端布局，支持触摸手势操作图表。
- **报告生成**: 支持一键生成 PDF/HTML 分析报告，包含截图和统计摘要。
